import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from typing import List, Optional

# ================================
# üßæ FUNCIONES UTILITARIAS
# ================================

def resumen_dataset(df: pd.DataFrame) -> None:
    """
    Muestra un resumen general del dataset: dimensiones, tipos de datos, nulos y estad√≠sticas descriptivas.
    """
    print(f"üîé Dimensiones: {df.shape}")
    print(f"\nüìã Tipos de datos:\n{df.dtypes}")
    print(f"\nüìâ Valores nulos:\n{df.isnull().sum()}")
    print(f"\nüìä Descripci√≥n:\n{df.describe(include='all').T}")


# =====================================================
# üö´ ELIMINAR COLUMNAS IRRELEVANTES AUTOM√ÅTICAMENTE
# =====================================================

def drop_irrelevant_columns(
    df: pd.DataFrame,
    protected_cols: Optional[List[str]] = None,
    manual_drop: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Elimina columnas innecesarias:
    - Completamente vac√≠as
    - Constantes (un √∫nico valor)
    - Indicadas manualmente
    Se respeta una lista de columnas protegidas que no deben eliminarse.
    """
    protected_cols = protected_cols or []
    manual_drop = manual_drop or []

    # Detectar columnas vac√≠as
    empty_cols = df.columns[df.isnull().all()].tolist()

    # Detectar columnas constantes
    constant_cols = df.columns[df.nunique() <= 1].tolist()

    # Unir todas las columnas a eliminar
    all_to_drop = list(set(empty_cols + constant_cols + manual_drop))

    # Excluir columnas protegidas
    final_to_drop = [col for col in all_to_drop if col in df.columns and col not in protected_cols]

    print("üìå Columnas a eliminar:")
    print(final_to_drop)

    # Eliminar columnas seleccionadas
    df = df.drop(columns=final_to_drop, errors='ignore')
    print("‚úÖ Limpieza de columnas irrelevantes completada.")
    return df


# ================================
# üßΩ ELIMINAR DUPLICADOS
# ================================

def eliminar_duplicados(df: pd.DataFrame) -> pd.DataFrame:
    """
    Elimina registros duplicados del DataFrame y muestra cu√°ntos se eliminaron.
    """
    before = df.shape[0]
    df = df.drop_duplicates()
    after = df.shape[0]
    print(f"üßπ Duplicados eliminados: {before - after}")
    return df


# ====================================================
# ‚öôÔ∏è IMPUTACI√ìN AUTOM√ÅTICA DE VALORES NULOS
# ====================================================

def imputar_valores_nulos(
    df: pd.DataFrame,
    protected_cols: Optional[List[str]] = None,
    threshold: float = 0.3
) -> pd.DataFrame:
    """
    Imputa valores nulos autom√°ticamente:
    - Num√©ricos: por la media si hay pocos nulos.
    - Categ√≥ricos: por la moda.
    Las columnas con demasiados nulos se omiten (seg√∫n el umbral).
    Tambi√©n se respeta una lista de columnas protegidas.
    """
    protected_cols = protected_cols or []

    for col in df.columns:
        if col in protected_cols:
            continue

        missing_ratio = df[col].isnull().mean()

        if missing_ratio == 0:
            continue
        elif missing_ratio > threshold:
            print(f"‚ö†Ô∏è Saltando imputaci√≥n en {col} (m√°s del {threshold*100:.1f}% de nulos).")
            continue

        # Imputaci√≥n seg√∫n tipo de dato
        if df[col].dtype in [np.float64, np.int64]:
            valor = df[col].mean()
            df[col] = df[col].fillna(valor)
            print(f"üîß Imputado {col} con media: {valor:.2f}")
        else:
            moda = df[col].mode().iloc[0]
            df[col] = df[col].fillna(moda)
            print(f"üîß Imputado {col} con moda: {moda}")

    return df


# ====================================================
# üî¢ ENCODING INTELIGENTE
# ====================================================

def codificar_variables(
    df: pd.DataFrame,
    protected_cols: Optional[List[str]] = None,
    max_onehot: int = 10
) -> pd.DataFrame:
    """
    Codifica variables categ√≥ricas autom√°ticamente:
    - Binarias: Label Encoding
    - Categ√≥ricas con pocos valores √∫nicos: One-hot Encoding
    - Alta cardinalidad: elimina la columna
    Se omiten las columnas protegidas.
    """
    protected_cols = protected_cols or []
    df_encoded = df.copy()
    label_encoder = LabelEncoder()

    for col in df_encoded.select_dtypes(include="object").columns:
        if col in protected_cols:
            continue

        unique_vals = df_encoded[col].nunique()

        if unique_vals == 2:
            # Codificaci√≥n para variables binarias
            df_encoded[col] = label_encoder.fit_transform(df_encoded[col])
            print(f"üîí Label encoded: {col}")

        elif unique_vals <= max_onehot:
            # One-hot encoding para pocas categor√≠as
            dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)
            df_encoded = pd.concat([df_encoded.drop(columns=[col]), dummies], axis=1)
            print(f"üîí One-hot encoded: {col}")

        else:
            # Demasiados valores √∫nicos: se elimina por alta cardinalidad
            df_encoded.drop(columns=[col], inplace=True)
            print(f"‚ö†Ô∏è Eliminado {col} por alta cardinalidad ({unique_vals})")

    return df_encoded


# ===============================================
# üìè ESCALADO DE VARIABLES NUM√âRICAS
# ===============================================

def escalar_columnas_numericas(
    df: pd.DataFrame,
    columnas: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Escala columnas num√©ricas utilizando StandardScaler:
    - Si no se especifican columnas, escala todas las num√©ricas.
    """
    scaler = StandardScaler()
    columnas = columnas or df.select_dtypes(include=np.number).columns.tolist()

    df[columnas] = scaler.fit_transform(df[columnas])
    print(f"üìê Escaladas columnas: {columnas}")
    return df


# ===============================================
# ‚úÇÔ∏è DIVISI√ìN EN TRAIN Y TEST
# ===============================================

def dividir_dataset(
    df: pd.DataFrame,
    target: str,
    test_size: float = 0.2,
    random_state: int = 42
):
    """
    Divide el dataset en conjuntos de entrenamiento y prueba,
    estratificando por la variable objetivo.
    """
    X = df.drop(columns=[target])
    y = df[target]
    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)
